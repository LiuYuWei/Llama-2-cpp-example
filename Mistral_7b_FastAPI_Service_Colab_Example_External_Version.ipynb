{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUC9Opfd8Mpt"
      },
      "source": [
        "# Llama 2 Fastapi Service Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9e9QOWD-l7k"
      },
      "source": [
        "推薦至少使用 T4 GPU 來作為你的服務啟用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4zPTxsL7rUn"
      },
      "source": [
        "## Step 0: Config Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8kZ_1n8U85GQ"
      },
      "outputs": [],
      "source": [
        "# 如果你想要在 Google Colab 長期測試，你可以使用 ngrok 來做服務器代理的處理\n",
        "# 到官方網站註冊帳號：https://ngrok.com/\n",
        "# 申請token，你就可以將以下變數進行更換。\n",
        "NGROK_TOKEN = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vru9EFFb9lmp"
      },
      "outputs": [],
      "source": [
        "# GGML Model\n",
        "# 你可以到 HuggingFace 去找尋相關的 GGML 模型\n",
        "# Example:\n",
        "# Llama: https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML\n",
        "# Taiwan Llama: https://huggingface.co/audreyt/Taiwan-LLaMa-v1.0-GGML\n",
        "GGML_HUGGINGFACE_REPO = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "GGML_HUGGINGFACE_BIN_FILE = \"mistral-7b-instruct-v0.1.Q5_0.gguf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXt_MjHG9Fw-"
      },
      "source": [
        "## Step 1: Install python package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV9rhIbg6K9d",
        "outputId": "244f7ecf-cea2-4f00-88dd-75e9d463bc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.8)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.0.0.tar.gz (718 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.7/718.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.5.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Collecting huggingface-hub (from accelerate)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.1.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Collecting huggingface-hub (from accelerate)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-7.0.0-py3-none-any.whl size=21129 sha256=8c52885b8244ce45c2b250a1f10c103a8aa80a25571b40ad3905d1076eaad0fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/29/7b/f64332aa7e5e88fbd56d4002185ae22dcdc83b35b3d1c2cbf5\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: safetensors, pyngrok, h11, uvicorn, starlette, huggingface-hub, tokenizers, fastapi, transformers, accelerate\n",
            "Successfully installed accelerate-0.23.0 fastapi-0.103.2 h11-0.14.0 huggingface-hub-0.17.3 pyngrok-7.0.0 safetensors-0.4.0 starlette-0.27.0 tokenizers-0.14.1 transformers-4.34.0 uvicorn-0.23.2\n"
          ]
        }
      ],
      "source": [
        "# 安裝 fastapi, nest-asyncio, pyngrok, uvicorn, accelerate 和 transformers 套件，以支援API開發和深度學習模型的操作。\n",
        "!pip install fastapi nest-asyncio pyngrok uvicorn accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yK3DJgd66El",
        "outputId": "523e2ccc-e343-4887-9f4e-4d15d7c7d5c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python==0.2.6\n",
            "  Downloading llama_cpp_python-0.2.6.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting scikit-build-core[pyproject]>=0.5.0\n",
            "    Downloading scikit_build_core-0.5.1-py3-none-any.whl (130 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.7/130.7 kB 3.3 MB/s eta 0:00:00\n",
            "  Collecting exceptiongroup (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "  Collecting packaging>=20.9 (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 4.9 MB/s eta 0:00:00\n",
            "  Collecting tomli>=1.1 (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "  Collecting pyproject-metadata>=0.5 (from scikit-build-core[pyproject]>=0.5.0)\n",
            "    Downloading pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n",
            "  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n",
            "  Successfully installed exceptiongroup-1.1.3 packaging-23.2 pathspec-0.11.2 pyproject-metadata-0.7.1 scikit-build-core-0.5.1 tomli-2.0.1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command pip subprocess to install backend dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting ninja>=1.5\n",
            "    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 307.2/307.2 kB 5.9 MB/s eta 0:00:00\n",
            "  Collecting cmake>=3.12\n",
            "    Downloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/26.0 MB 64.1 MB/s eta 0:00:00\n",
            "  Installing collected packages: ninja, cmake\n",
            "    Creating /tmp/pip-build-env-jtzorgni/normal/local/bin\n",
            "    changing mode of /tmp/pip-build-env-jtzorgni/normal/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-jtzorgni/normal/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-jtzorgni/normal/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-jtzorgni/normal/local/bin/ctest to 755\n",
            "  Successfully installed cmake-3.27.6 ninja-1.11.1.1\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  *** scikit-build-core 0.5.1 using CMake 3.27.6 (metadata_wheel)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.6)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.6)\n",
            "  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.6)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m168.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "  *** scikit-build-core 0.5.1 using CMake 3.27.6 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  loading initial cache file /tmp/tmp7nb_ugyd/build/CMakeInit.txt\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  fatal: not a git repository (or any of the parent directories): .git\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:125 (message):\n",
            "    Git repository not found; to enable automatic generation of build info,\n",
            "    make sure Git is installed and the project is a Git repository.\n",
            "\n",
            "\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/include (found version \"11.8.89\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 11.8.89\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:19 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:28 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Configuring done (4.1s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/tmp7nb_ugyd/build\n",
            "  *** Building project with Ninja...\n",
            "  Change Dir: '/tmp/tmp7nb_ugyd/build'\n",
            "\n",
            "  Run Build Command(s): /tmp/pip-build-env-jtzorgni/normal/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -v\n",
            "  [1/11] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/ggml-alloc.c\n",
            "  [2/11] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o -c /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/k_quants.c\n",
            "  [3/11] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/ggml.c\n",
            "  [4/11] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/common/. -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/common/common.cpp\n",
            "  [5/11] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/common/. -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/common/console.cpp\n",
            "  [6/11] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/llama.cpp\n",
            "  [7/11] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/common/. -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/common/grammar-parser.cpp\n",
            "  [8/11] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_USE_CUBLAS -DGGML_USE_K_QUANTS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -mf16c -mfma -mavx -mavx2 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  [9/11] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-11.8/targets/x86_64-linux/lib:  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n",
            "  [10/11] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\" && :\n",
            "  [11/11] : && /tmp/pip-build-env-jtzorgni/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n",
            "\n",
            "  *** Installing project into wheel...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/include/ggml.h\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/include/ggml-cuda.h\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/include/k_quants.h\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/lib/libllama.so\n",
            "  -- Set runtime path of \"/tmp/tmp7nb_ugyd/wheel/platlib/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/include/llama.h\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/bin/convert.py\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/tmp7nb_ugyd/wheel/platlib/llama_cpp/libllama.so\n",
            "  -- Set runtime path of \"/tmp/tmp7nb_ugyd/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/llama_cpp/libllama.so\n",
            "  -- Set runtime path of \"/tmp/pip-install-b_58w3ro/llama-cpp-python_fea363a341424390b40ac8f0e947b184/llama_cpp/libllama.so\" to \"\"\n",
            "  *** Making wheel...\n",
            "  *** Created llama_cpp_python-0.2.6-cp310-cp310-manylinux_2_35_x86_64.whl...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.6-cp310-cp310-manylinux_2_35_x86_64.whl size=6197645 sha256=db069656f0ddd0a132b2293fac9594d8453103881145b0de6cecb9479c580568\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0s3mev29/wheels/6c/ae/75/c2ad88ef0d1e219f981c51367b8533025345d1a14aa2f09662\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.5.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/bin/f2py3\n",
            "      Removing file or directory /usr/local/bin/f2py3.10\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.23.5.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.6 numpy-1.26.0 typing-extensions-4.8.0\n"
          ]
        }
      ],
      "source": [
        "# 安裝特定版本的 llama-cpp-python 套件，並啟用 CUDA 的 cuBLAS 功能。\n",
        "# `--force-reinstall` 會強制重新安裝，`--upgrade` 會確保安裝最新版本，而 `--no-cache-dir` 會避免使用本地快取，`--verbose` 提供詳細的輸出。\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.6 --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRoUDXE666sk",
        "outputId": "21f27fac-f8ad-4bec-d025-4aaa612572fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# 安裝 huggingface_hub 套件，此套件可支援與 Hugging Face Model Hub 進行交互。\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ye1-jg7x8r"
      },
      "source": [
        "## Step 2: Download GGML Model and predict the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u4LMufzw6jUU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# 設定日誌\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.loaded = False        # 模型是否已經加載的標志\n",
        "        self.lcpp_llm = None       # 儲存 Llama 模型的變數\n",
        "        self.model_path = \"\"       # 模型的路徑\n",
        "\n",
        "    def load(self, model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\", model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"):\n",
        "        # 從 Hugging Face Model Hub 下載模型並設定其路徑\n",
        "        self.model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "        logger.info(\"Finish: Load Llama 2 model.\")  # 輸出模型加載完成的信息\n",
        "\n",
        "    def predict(self, data):\n",
        "        # 如果模型還沒有被加載，則加載模型\n",
        "        if not self.loaded:\n",
        "            self.loaded = True\n",
        "            self.lcpp_llm = Llama(\n",
        "                model_path=self.model_path,\n",
        "                n_threads=2,             # 使用的執行緒數量\n",
        "                n_batch=1024,            # 批次大小\n",
        "                n_gpu_layers=32          # 使用的GPU層數\n",
        "            )\n",
        "        logger.info(\"========== Start ==========\")\n",
        "        # 將 JSON 字符串反序列化成字典\n",
        "        # data_dict = json.loads(data)\n",
        "        # logger.info(\"Input: {}.\".format(data_dict))\n",
        "        # 使用 Llama 模型進行預測\n",
        "        response = self.lcpp_llm.create_chat_completion(**data)\n",
        "        # logger.info(\"Response: {}.\".format(response))\n",
        "        logger.info(\"==========  End  ==========\")\n",
        "\n",
        "        return response  # 返回模型的預測結果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8f29020ee2804d62b2d4ddf5c5e9dfc7",
            "fc0fe3ba1ded40928b6a6c8aea1acd03",
            "00650e8e0ae24b02b683db4736bada70",
            "d010932c0a49445b8ac2d8baef470e2a",
            "06e62ad5a52642229923c5c270e814a3",
            "002be6b865234b6f9e9a15ea410a5985",
            "95a6d7f2d99849c08f8405e995b6f2ce",
            "ffae03bf30e749fa87e81978357d47df",
            "0de5e720445e419aaea22dcb957fe950",
            "dd5a13e7d1454397a50e7ef4d29a9b67",
            "c7da60c7cdaf46cca4eb87fb385f78da"
          ]
        },
        "id": "uiHZFZGf6kSd",
        "outputId": "1ff319e9-6b92-477c-a2bb-d8e673cdfec0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)truct-v0.1.Q5_0.gguf:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f29020ee2804d62b2d4ddf5c5e9dfc7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_instance = Model()\n",
        "model_instance.load(model_name_or_path = GGML_HUGGINGFACE_REPO, model_basename = GGML_HUGGINGFACE_BIN_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"你是一個有幫助的問答機器人，請用繁體中文回覆。\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"台灣首都在哪裏？\"\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "XMAyC2ijkQgr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_instance.predict(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NSVUPVDiy81",
        "outputId": "afa86862-f741-48e5-b151-8d8d0590fb29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-b82e3519-f0d6-4c56-80ab-9c3d52ee8886',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1697093944,\n",
              " 'model': '/root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/45167a542b6fa64a14aea61a4c468bbbf9f258a8/mistral-7b-instruct-v0.1.Q5_0.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant', 'content': '台灣首都在台北。'},\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 45, 'completion_tokens': 8, 'total_tokens': 53}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W4IKvhb7-E4"
      },
      "source": [
        "## Step 3: Build the fastapi service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BoM5mJIe6kP7"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "# 初始化 FastAPI 應用\n",
        "app = FastAPI()\n",
        "\n",
        "# 為 FastAPI 應用加入 CORS 中間件，允許跨域請求\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],             # 允許所有來源的跨域請求\n",
        "    allow_credentials=True,          # 允許憑證（例如 cookies、HTTP認證）的傳遞\n",
        "    allow_methods=['*'],             # 允許所有的 HTTP 方法\n",
        "    allow_headers=['*'],             # 允許所有的 HTTP 頭部\n",
        ")\n",
        "\n",
        "@app.post(\"/predict\")                # 定義一個 POST 路由，用於模型預測\n",
        "async def predict_text(json_input: dict):  # 接收一個字典格式的 JSON 輸入\n",
        "    result = model_instance.predict(json_input)  # 使用模型實例進行預測\n",
        "    return result                           # 返回預測結果\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ7WeIyg8C73"
      },
      "source": [
        "## Step 4: Start the fastapi service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMC363JK82jq",
        "outputId": "fdbc56ae-88ca-4f95-add2-4ca5e92a7a89"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-10-12T06:59:06+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n",
            "INFO:     Started server process [646]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://8dd9-104-199-121-204.ngrok-free.app\n",
            "You can use https://8dd9-104-199-121-204.ngrok-free.app/predict to get the assistant result.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     35.229.163.27:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# 設定 ngrok 的授權令牌\n",
        "if NGROK_TOKEN is not None:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# 建立與 ngrok 的隧道，使外部可以訪問本地的 8000 端口\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "public_url = ngrok_tunnel.public_url\n",
        "\n",
        "print('Public URL:', public_url)  # 輸出公開的 URL\n",
        "print(\"You can use {}/predict to get the assistant result.\".format(public_url))\n",
        "\n",
        "\n",
        "# 使用 nest_asyncio 修正異步事件循環的問題\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 啟動 uvicorn 伺服器，使 FastAPI 應用運行在 8000 端口\n",
        "uvicorn.run(app, port=8000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuWit7ePf2om"
      },
      "source": [
        "### Example CURL command line:\n",
        "\n",
        "```bash\n",
        "curl --location 'https://f1b8-35-184-42-82.ngrok-free.app/predict' \\\n",
        "--header 'Content-Type: application/json' \\\n",
        "--data '{\"prompt\": \"test\", \"max_tokens\": 2}'\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f29020ee2804d62b2d4ddf5c5e9dfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc0fe3ba1ded40928b6a6c8aea1acd03",
              "IPY_MODEL_00650e8e0ae24b02b683db4736bada70",
              "IPY_MODEL_d010932c0a49445b8ac2d8baef470e2a"
            ],
            "layout": "IPY_MODEL_06e62ad5a52642229923c5c270e814a3"
          }
        },
        "fc0fe3ba1ded40928b6a6c8aea1acd03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002be6b865234b6f9e9a15ea410a5985",
            "placeholder": "​",
            "style": "IPY_MODEL_95a6d7f2d99849c08f8405e995b6f2ce",
            "value": "Downloading (…)truct-v0.1.Q5_0.gguf: 100%"
          }
        },
        "00650e8e0ae24b02b683db4736bada70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffae03bf30e749fa87e81978357d47df",
            "max": 4997715616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0de5e720445e419aaea22dcb957fe950",
            "value": 4997715616
          }
        },
        "d010932c0a49445b8ac2d8baef470e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd5a13e7d1454397a50e7ef4d29a9b67",
            "placeholder": "​",
            "style": "IPY_MODEL_c7da60c7cdaf46cca4eb87fb385f78da",
            "value": " 5.00G/5.00G [00:28&lt;00:00, 91.2MB/s]"
          }
        },
        "06e62ad5a52642229923c5c270e814a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "002be6b865234b6f9e9a15ea410a5985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95a6d7f2d99849c08f8405e995b6f2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffae03bf30e749fa87e81978357d47df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0de5e720445e419aaea22dcb957fe950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd5a13e7d1454397a50e7ef4d29a9b67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7da60c7cdaf46cca4eb87fb385f78da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}